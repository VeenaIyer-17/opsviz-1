NAME: test1
LAST DEPLOYED: Tue Mar 10 21:45:09 2020
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
elasticsearch:
  antiAffinity: hard
  antiAffinityTopologyKey: kubernetes.io/hostname
  clusterHealthCheckParams: wait_for_status=green&timeout=1s
  clusterName: elasticsearch
  esConfig: {}
  esJavaOpts: -Xmx1g -Xms1g
  esMajorVersion: ""
  extraContainers: ""
  extraEnvs: []
  extraInitContainers: ""
  extraVolumeMounts: ""
  extraVolumes: ""
  fsGroup: ""
  fullnameOverride: ""
  global: {}
  httpPort: 9200
  image: docker.elastic.co/elasticsearch/elasticsearch
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []
  imageTag: 7.6.1
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - chart-example.local
    path: /
    tls: []
  initResources: {}
  keystore: []
  labels: {}
  lifecycle: {}
  masterService: ""
  masterTerminationFix: false
  maxUnavailable: 1
  minimumMasterNodes: 2
  nameOverride: ""
  networkHost: 0.0.0.0
  nodeAffinity: {}
  nodeGroup: master
  nodeSelector: {}
  persistence:
    annotations: {}
    enabled: true
  podAnnotations: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    fsGroup: 1000
    runAsUser: 1000
  podSecurityPolicy:
    create: false
    name: ""
    spec:
      fsGroup:
        rule: RunAsAny
      privileged: true
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
      - secret
      - configMap
      - persistentVolumeClaim
  priorityClassName: ""
  protocol: http
  rbac:
    create: false
    serviceAccountName: ""
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5
  replicas: 3
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 2Gi
  roles:
    data: "true"
    ingest: "true"
    master: "true"
  schedulerName: ""
  secretMounts: []
  securityContext:
    capabilities:
      drop:
      - ALL
    runAsNonRoot: true
    runAsUser: 1000
  service:
    annotations: {}
    httpPortName: http
    labels: {}
    labelsHeadless: {}
    loadBalancerSourceRanges: []
    nodePort: ""
    transportPortName: transport
    type: ClusterIP
  sidecarResources: {}
  sysctlInitContainer:
    enabled: true
  sysctlVmMaxMapCount: 262144
  terminationGracePeriod: 120
  tolerations: []
  transportPort: 9300
  updateStrategy: RollingUpdate
  volumeClaimTemplate:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 30Gi
fluentd:
  affinity: {}
  annotations: {}
  autoscaling:
    enabled: false
    maxReplicas: 5
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 60
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 60
          type: Utilization
      type: Resource
    minReplicas: 2
  configMaps:
    forward-input.conf: |
      <source>
        @type forward
        port 24224
        bind 0.0.0.0
      </source>
    general.conf: |
      # Prevent fluentd from handling records containing its own logs. Otherwise
      # it can lead to an infinite loop, when error in sending one message generates
      # another message which also fails to be sent and so on.
      <match fluentd.**>
        @type null
      </match>

      # Used for health checking
      <source>
        @type http
        port 9880
        bind 0.0.0.0
      </source>

      # Emits internal metrics to every minute, and also exposes them on port
      # 24220. Useful for determining if an output plugin is retryring/erroring,
      # or determining the buffer queue length.
      <source>
        @type monitor_agent
        bind 0.0.0.0
        port 24220
        tag fluentd.monitor.metrics
      </source>
    output.conf: |
      <match **>
        @id elasticsearch
        @type elasticsearch
        @log_level info
        include_tag_key true
        # Replace with the host/port to your Elasticsearch cluster.
        host "#{ENV['OUTPUT_HOST']}"
        port "#{ENV['OUTPUT_PORT']}"
        scheme "#{ENV['OUTPUT_SCHEME']}"
        ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
        logstash_format true
        <buffer>
          @type file
          path /var/log/fluentd-buffers/kubernetes.system.buffer
          flush_mode interval
          retry_type exponential_backoff
          flush_thread_count 2
          flush_interval 5s
          retry_forever
          retry_max_interval 30
          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
          queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
          overflow_action block
        </buffer>
      </match>
    system.conf: |-
      <system>
        root_dir /tmp/fluentd-buffers/
      </system>
  env: {}
  global: {}
  image:
    pullPolicy: IfNotPresent
    repository: gcr.io/google-containers/fluentd-elasticsearch
    tag: v2.4.0
  ingress:
    annotations:
      kubernetes.io/ingress.class: nginx
    enabled: false
    hosts: null
    labels: []
    tls: {}
  metrics:
    enabled: false
    service:
      port: 24231
    serviceMonitor:
      additionalLabels: {}
      enabled: false
  nodeSelector: {}
  output:
    buffer_chunk_limit: 2M
    buffer_queue_limit: 8
    host: elasticsearch-client.default.svc.cluster.local
    port: 9200
    scheme: http
    sslVersion: TLSv1
  persistence:
    accessMode: ReadWriteOnce
    enabled: false
    size: 10Gi
  plugins:
    enabled: false
    pluginsList: []
  rbac:
    create: true
  resources: {}
  service:
    annotations: {}
    ports:
    - containerPort: 24220
      name: monitor-agent
      protocol: TCP
    type: ClusterIP
  serviceAccount:
    create: true
    name: null
  terminationGracePeriodSeconds: 30
  tolerations: []
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: nginx
imagePullSecrets: []
ingress:
  annotations: {}
  enabled: false
  hosts:
  - host: chart-example.local
    paths: []
  tls: []
kibana:
  affinity: {}
  elasticsearchHosts: http://elasticsearch-master:9200
  elasticsearchURL: ""
  extraContainers: ""
  extraEnvs: []
  extraInitContainers: ""
  fullnameOverride: ""
  global: {}
  healthCheckPath: /app/kibana
  httpPort: 5601
  image: docker.elastic.co/kibana/kibana
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []
  imageTag: 7.6.1
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - chart-example.local
    path: /
    tls: []
  kibanaConfig: {}
  labels: {}
  lifecycle: {}
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext:
    fsGroup: 1000
  priorityClassName: ""
  protocol: http
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5
  replicas: 1
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 500Mi
  secretMounts: []
  securityContext:
    capabilities:
      drop:
      - ALL
    runAsNonRoot: true
    runAsUser: 1000
  serverHost: 0.0.0.0
  service:
    annotations: {}
    labels: {}
    loadBalancerSourceRanges: []
    nodePort: ""
    port: 5601
    type: ClusterIP
  serviceAccount: ""
  tolerations: []
  updateStrategy:
    type: Recreate
nameOverride: ""
nodeSelector: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 80
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: null
tolerations: []

HOOKS:
---
# Source: opsviz-chart/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "test1-nnuey-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "test1-wwllf-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.1"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'
  restartPolicy: Never
---
# Source: opsviz-chart/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "test1-opsviz-chart-test-connection"
  labels:
    helm.sh/chart: opsviz-chart-0.1.0
    app.kubernetes.io/name: opsviz-chart
    app.kubernetes.io/instance: test1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['test1-opsviz-chart:80']
  restartPolicy: Never
MANIFEST:
---
# Source: opsviz-chart/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: "elasticsearch-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "elasticsearch-master"
---
# Source: opsviz-chart/charts/fluentd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
---
# Source: opsviz-chart/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test1-opsviz-chart
  labels:
    helm.sh/chart: opsviz-chart-0.1.0
    app.kubernetes.io/name: opsviz-chart
    app.kubernetes.io/instance: test1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: opsviz-chart/charts/fluentd/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
data:
  forward-input.conf: |-
    <source>
      @type forward
      port 24224
      bind 0.0.0.0
    </source>
    
  general.conf: |-
    # Prevent fluentd from handling records containing its own logs. Otherwise
    # it can lead to an infinite loop, when error in sending one message generates
    # another message which also fails to be sent and so on.
    <match fluentd.**>
      @type null
    </match>
    
    # Used for health checking
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    
    # Emits internal metrics to every minute, and also exposes them on port
    # 24220. Useful for determining if an output plugin is retryring/erroring,
    # or determining the buffer queue length.
    <source>
      @type monitor_agent
      bind 0.0.0.0
      port 24220
      tag fluentd.monitor.metrics
    </source>
    
  output.conf: |-
    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      include_tag_key true
      # Replace with the host/port to your Elasticsearch cluster.
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>
    
  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
---
# Source: opsviz-chart/charts/fluentd/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
rules:
- apiGroups:
  - ""
  resources:
  - "namespaces"
  - "pods"
  verbs:
  - "get"
  - "watch"
  - "list"
---
# Source: opsviz-chart/charts/fluentd/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
subjects:
- kind: ServiceAccount
  name: test1-fluentd
  namespace: default
roleRef:
  kind: ClusterRole
  name: test1-fluentd
  apiGroup: rbac.authorization.k8s.io
---
# Source: opsviz-chart/charts/fluentd/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - test1-fluentd
---
# Source: opsviz-chart/charts/fluentd/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
roleRef:
  kind: Role
  name: test1-fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: test1-fluentd
  namespace: default
---
# Source: opsviz-chart/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "test1"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    heritage: "Helm"
    release: "test1"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: opsviz-chart/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch-master-headless
  labels:
    heritage: "Helm"
    release: "test1"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "elasticsearch-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: opsviz-chart/charts/fluentd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  
  ports:
    - name: monitor-agent
      port: 24220
      targetPort: 24220
      protocol: TCP
  
  selector:
    app: fluentd
    release: test1
---
# Source: opsviz-chart/charts/kibana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test1-kibana
  labels:
    app: kibana
    release: "test1"
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5601
      protocol: TCP
      name: http
      targetPort: 5601
  selector:
    app: kibana
    release: "test1"
---
# Source: opsviz-chart/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test1-opsviz-chart
  labels:
    helm.sh/chart: opsviz-chart-0.1.0
    app.kubernetes.io/name: opsviz-chart
    app.kubernetes.io/instance: test1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: opsviz-chart
    app.kubernetes.io/instance: test1
---
# Source: opsviz-chart/charts/fluentd/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test1-fluentd
  labels:
    app: fluentd
    chart: fluentd-2.4.0
    release: test1
    heritage: Helm
spec:
  replicas: 
  selector:
    matchLabels:
      app: fluentd
      release: test1
  template:
    metadata:
      labels:
        app: fluentd
        release: test1
      annotations:
        checksum/configmap: d5d096104d706b1abe4138a5eaf72521d56929244889056fca77c8b32a2ad312
    spec:
      terminationGracePeriodSeconds: 30
      containers:
      - name: fluentd
        image: "gcr.io/google-containers/fluentd-elasticsearch:v2.4.0"
        imagePullPolicy: IfNotPresent
        env:
          - name: OUTPUT_HOST
            value: "elasticsearch-client.default.svc.cluster.local"
          - name: OUTPUT_PORT
            value: "9200"
          - name: OUTPUT_SCHEME
            value: "http"
          - name: OUTPUT_SSL_VERSION
            value: "TLSv1"
          - name: OUTPUT_BUFFER_CHUNK_LIMIT
            value: "2M"
          - name: OUTPUT_BUFFER_QUEUE_LIMIT
            value: "8"
        resources:
            {}
        ports:
          - name: monitor-agent
            containerPort: 24220
            protocol: TCP
          - name: http-input
            containerPort: 9880
            protocol: TCP
        livenessProbe:
          httpGet:
            # Use percent encoding for query param.
            # The value is {"log": "health check"}.
            # the endpoint itself results in a new fluentd
            # tag 'fluentd.pod-healthcheck'
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 5
          timeoutSeconds: 1
        volumeMounts:
        - name: config-volume-test1-fluentd
          mountPath: /etc/fluent/config.d
        - name: buffer
          mountPath: "/var/log/fluentd-buffers"
      serviceAccountName: test1-fluentd
      volumes:
        - name: config-volume-test1-fluentd
          configMap:
            name: test1-fluentd
            defaultMode: 0777
        - name: buffer
          emptyDir: {}
---
# Source: opsviz-chart/charts/kibana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test1-kibana
  labels:
    app: kibana
    release: "test1"
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: kibana
      release: "test1"
  template:
    metadata:
      labels:
        app: kibana
        release: "test1"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
      volumes:
      containers:
      - name: kibana
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/kibana/kibana:7.6.1"
        imagePullPolicy: "IfNotPresent"
        env:
          - name: ELASTICSEARCH_HOSTS
            value: "http://elasticsearch-master:9200"
          - name: SERVER_HOST
            value: "0.0.0.0"
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                http () {
                    local path="${1}"
                    set -- -XGET -s --fail

                    if [ -n "${ELASTICSEARCH_USERNAME}" ] && [ -n "${ELASTICSEARCH_PASSWORD}" ]; then
                      set -- "$@" -u "${ELASTICSEARCH_USERNAME}:${ELASTICSEARCH_PASSWORD}"
                    fi

                    STATUS=$(curl --output /dev/null --write-out "%{http_code}" -k "$@" "http://localhost:5601${path}")
                    if [[ "${STATUS}" -eq 200 ]]; then
                      exit 0
                    fi

                    echo "Error: Got HTTP code ${STATUS} but expected a 200"
                    exit 1
                }

                http "/app/kibana"
        ports:
        - containerPort: 5601
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 500Mi
        volumeMounts:
---
# Source: opsviz-chart/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test1-opsviz-chart
  labels:
    helm.sh/chart: opsviz-chart-0.1.0
    app.kubernetes.io/name: opsviz-chart
    app.kubernetes.io/instance: test1
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opsviz-chart
      app.kubernetes.io/instance: test1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opsviz-chart
        app.kubernetes.io/instance: test1
    spec:
      serviceAccountName: test1-opsviz-chart
      securityContext:
        {}
      containers:
        - name: opsviz-chart
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: opsviz-chart/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-master
  labels:
    heritage: "Helm"
    release: "test1"
    chart: "elasticsearch"
    app: "elasticsearch-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: elasticsearch-master-headless
  selector:
    matchLabels:
      app: "elasticsearch-master"
  replicas: 3
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 30Gi
  template:
    metadata:
      name: "elasticsearch-master"
      labels:
        heritage: "Helm"
        release: "test1"
        chart: "elasticsearch"
        app: "elasticsearch-master"
      annotations:
        
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "elasticsearch-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.1"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.1"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=green&timeout=1s' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                      BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                    else
                      BASIC_AUTH=''
                    fi
                    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                    http "/_cluster/health?timeout=0s"
                else
                    echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=green&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=green&timeout=1s" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "wait_for_status=green&timeout=1s" )'
                        exit 1
                    fi
                fi
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,"
          - name: discovery.seed_hosts
            value: "elasticsearch-master-headless"
          - name: cluster.name
            value: "elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx1g -Xms1g"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
        volumeMounts:
          - name: "elasticsearch-master"
            mountPath: /usr/share/elasticsearch/data

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=opsviz-chart,app.kubernetes.io/instance=test1" -o jsonpath="{.items[0].metadata.name}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:80
